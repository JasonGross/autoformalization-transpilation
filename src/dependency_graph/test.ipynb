{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDpdFile(filePath: str) -> Tuple[Dict[str, Dict[str, str]], List[Tuple[str, str]]]:\n",
    "    nodes: Dict[str, Dict[str, str]] = {}\n",
    "    edges: List[Tuple[str, str]] = []\n",
    "\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"N:\"):\n",
    "                parts = line.split(maxsplit=3)\n",
    "                nodeId: str = parts[1]\n",
    "                label: str = parts[2].strip('\"')\n",
    "                attributes: str = parts[3] if len(parts) > 3 else \"\"\n",
    "                nodes[nodeId] = {\"label\": label, \"attributes\": attributes}\n",
    "            elif line.startswith(\"E:\"):\n",
    "                parts = line.split()\n",
    "                source: str = parts[1]\n",
    "                target: str = parts[2]\n",
    "                edges.append((source, target))\n",
    "\n",
    "    return nodes, edges\n",
    "\n",
    "def createGraphFromDpd(filePath: str) -> nx.DiGraph:\n",
    "    nodes, edges = parseDpdFile(filePath)\n",
    "    G = nx.DiGraph()\n",
    "    for nodeId, data in nodes.items():\n",
    "        G.add_node(nodeId, **data)\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "def get_second_word(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    tokens = text.split()\n",
    "    if len(tokens) >= 2:\n",
    "        return tokens[1].replace(':', '')\n",
    "    return \"\"\n",
    "\n",
    "def parse_attributes(attr_str: str) -> dict:\n",
    "    attr_str = attr_str.strip(\"[];\")\n",
    "    attr_pairs = attr_str.split(\", \")\n",
    "    attr_dict = {}\n",
    "    for pair in attr_pairs:\n",
    "        if \"=\" in pair:\n",
    "            key, value = pair.split(\"=\")\n",
    "            attr_dict[key.strip()] = value.strip().strip('\"')\n",
    "    return attr_dict\n",
    "\n",
    "def extract_filename(path: str) -> str:\n",
    "    if not path:\n",
    "        return \"\"\n",
    "    return path.split(\".\")[0] + \".v\"\n",
    "\n",
    "def main():\n",
    "    # Adjust BASE_DIR to be the parent of the current directory (i.e., src)\n",
    "    BASE_DIR: str = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    \n",
    "    # Build the paths relative to BASE_DIR\n",
    "    dpdDir: str = os.path.join(BASE_DIR, \"dataset\", \"dependency_graph\")\n",
    "    dpdFilePath: str = os.path.join(dpdDir, \"dgraph.dpd\")\n",
    "    \n",
    "    if not os.path.exists(dpdDir):\n",
    "        print(f\"Warning: Directory '{dpdDir}' does not exist. Creating it now.\")\n",
    "        os.makedirs(dpdDir, exist_ok=True)\n",
    "        print(f\"Created missing directory: {dpdDir}\")\n",
    "\n",
    "    # Directory for processed data output and Coq proofs dataset\n",
    "    processedDataDir: str = os.path.join(BASE_DIR, \"dataset\", \"processed_data\")\n",
    "    if not os.path.exists(processedDataDir):\n",
    "        print(f\"Warning: Directory '{processedDataDir}' does not exist. Creating it now.\")\n",
    "        os.makedirs(processedDataDir, exist_ok=True)\n",
    "        print(f\"Created missing directory: {processedDataDir}\")\n",
    "\n",
    "    coqDatasetPath: str = os.path.join(BASE_DIR, \"dataset\", \"processed_data\", \"coq_proofs_dataset.json\")\n",
    "    output_file: str = os.path.join(processedDataDir, \"df.json\")\n",
    "\n",
    "    # Create graph from the dependency graph file\n",
    "    G = createGraphFromDpd(dpdFilePath)\n",
    "\n",
    "    # Load the Coq proofs dataset\n",
    "    with open(coqDatasetPath, \"r\", encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    data = []\n",
    "    for entry in json_data:\n",
    "        file_name = entry.get(\"fileName\", \"\")\n",
    "        items = entry.get(\"items\", [])\n",
    "        for item in items:\n",
    "            raw_text = item.get(\"raw\", \"\")\n",
    "            item_type = item.get(\"type\", \"\")\n",
    "            data.append({\"fileName\": file_name, \"type\": item_type, \"raw\": raw_text})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Label\"] = df[\"raw\"].apply(get_second_word)\n",
    "\n",
    "    # Process nodes from the dependency graph\n",
    "    expanded_data = []\n",
    "    for node, attributes in G.nodes(data=True):\n",
    "        attrs = parse_attributes(attributes[\"attributes\"])\n",
    "        expanded_data.append({\"Node\": node, \"Label\": attributes[\"label\"], **attrs})\n",
    "\n",
    "    df_dpd = pd.DataFrame(expanded_data)\n",
    "    \n",
    "    # Check if 'path' column exists before using it\n",
    "    if \"path\" in df_dpd.columns:\n",
    "        df_dpd[\"file\"] = df_dpd[\"path\"].apply(extract_filename)\n",
    "    else:\n",
    "        df_dpd[\"file\"] = \"\"\n",
    "\n",
    "    merged_df = df.merge(\n",
    "        df_dpd[[\"file\", \"Label\", \"Node\"]],\n",
    "        left_on=[\"fileName\", \"Label\"],\n",
    "        right_on=[\"file\", \"Label\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged_df.drop(columns=[\"file\"], inplace=True)\n",
    "    merged_df[\"Node\"] = merged_df[\"Node\"].fillna(pd.NA)\n",
    "\n",
    "    df_merged = merged_df.dropna(subset=[\"Node\"])\n",
    "    if \"level\" in df_merged.columns:\n",
    "        df_merged = df_merged.sort_values(by=[\"level\", \"fileName\"])\n",
    "    else:\n",
    "        df_merged = df_merged.sort_values(by=[\"fileName\"])\n",
    "\n",
    "    df_merged.to_json(output_file, orient=\"records\", indent=2)\n",
    "    print(f\"Data successfully stored in JSON format at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Adjust BASE_DIR to be the parent of the current directory (i.e., src)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     BASE_DIR: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Build the paths relative to BASE_DIR\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     dpdDir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdependency_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
